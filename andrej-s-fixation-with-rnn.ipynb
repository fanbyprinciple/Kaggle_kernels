{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fanbyprinciple/andrej-s-fixation-with-rnn?scriptVersionId=92291273\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"2b3c2a81","metadata":{"papermill":{"duration":0.020012,"end_time":"2022-04-06T18:08:53.3593","exception":false,"start_time":"2022-04-06T18:08:53.339288","status":"completed"},"tags":[]},"source":["# Recurrent Neural Networks\n","Sequences. Depending on your background you might be wondering: What makes Recurrent Networks so special? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:\n","\n","![](http://karpathy.github.io/assets/rnn/diags.jpeg)"]},{"cell_type":"markdown","id":"daef10ab","metadata":{"papermill":{"duration":0.018141,"end_time":"2022-04-06T18:08:53.396246","exception":false,"start_time":"2022-04-06T18:08:53.378105","status":"completed"},"tags":[]},"source":["Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.\n","As you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of computational steps, and hence also much more appealing for those of us who aspire to build more intelligent systems. Moreover, as we’ll see in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that RNNs are Turing-Complete in the sense that they can to simulate arbitrary programs (with proper weights). But similar to universal approximation theorems for neural nets you shouldn’t read too much into this. In fact, forget I said anything.\n","\n","If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.\n","\n","Sequential processing in absence of sequences. You might be thinking that having sequences as inputs or outputs could be relatively rare, but an important point to realize is that even if your inputs/outputs are fixed vectors, it is still possible to use this powerful formalism to process them in a sequential manner. For instance, the figure below shows results from two very nice papers from DeepMind. On the left, an algorithm learns a recurrent network policy that steers its attention around an image; In particular, it learns to read out house numbers from left to right (Ba et al.). "]},{"cell_type":"code","execution_count":1,"id":"3531fb0c","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-06T18:08:53.44177Z","iopub.status.busy":"2022-04-06T18:08:53.441072Z","iopub.status.idle":"2022-04-06T18:08:53.448122Z","shell.execute_reply":"2022-04-06T18:08:53.44869Z","shell.execute_reply.started":"2022-04-06T17:59:01.654005Z"},"papermill":{"duration":0.034256,"end_time":"2022-04-06T18:08:53.449051","exception":false,"start_time":"2022-04-06T18:08:53.414795","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","class RNN:\n","    def __init__(self):\n","        self.W_hh = 1\n","        self.h = 1\n","        self.W_xh = 1\n","        self.W_hy = 1\n","        \n","    def step(self, x):\n","        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n","        y = np.dot(self.W_hy, self.h)\n","        return y"]},{"cell_type":"code","execution_count":2,"id":"df1898ea","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:53.489079Z","iopub.status.busy":"2022-04-06T18:08:53.488364Z","iopub.status.idle":"2022-04-06T18:08:53.493637Z","shell.execute_reply":"2022-04-06T18:08:53.494236Z","shell.execute_reply.started":"2022-04-06T17:59:01.667994Z"},"papermill":{"duration":0.027027,"end_time":"2022-04-06T18:08:53.494452","exception":false,"start_time":"2022-04-06T18:08:53.467425","status":"completed"},"tags":[]},"outputs":[],"source":["x = 1\n","y = 1\n","rnn = RNN()\n","y = rnn.step(x)"]},{"cell_type":"markdown","id":"1343c7f7","metadata":{"papermill":{"duration":0.018087,"end_time":"2022-04-06T18:08:53.531129","exception":false,"start_time":"2022-04-06T18:08:53.513042","status":"completed"},"tags":[]},"source":["The above specifies the forward pass of a vanilla RNN. This RNN’s parameters are the three matrices W_hh, W_xh, W_hy. The hidden state self.h is initialized with the zero vector. The np.tanh function implements a non-linearity that squashes the activations to the range [-1, 1]. Notice briefly how this works: There are two terms inside of the tanh: one is based on the previous hidden state and one is based on the current input. In numpy np.dot is matrix multiplication. The two intermediates interact with addition, and then get squashed by the tanh into the new state vector. If you’re more comfortable with math notation, we can also write the hidden state update as ht=tanh(Whhht−1+Wxhxt), where tanh is applied elementwise."]},{"cell_type":"markdown","id":"52e30f8c","metadata":{"papermill":{"duration":0.018126,"end_time":"2022-04-06T18:08:53.56767","exception":false,"start_time":"2022-04-06T18:08:53.549544","status":"completed"},"tags":[]},"source":["We initialize the matrices of the RNN with random numbers and the bulk of work during training goes into finding the matrices that give rise to desirable behavior, as measured with some loss function that expresses your preference to what kinds of outputs y you’d like to see in response to your input sequences x.\n","\n","Going deep. RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes. For instance, we can form a 2-layer recurrent network as follows:"]},{"cell_type":"code","execution_count":3,"id":"927662e1","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:53.60989Z","iopub.status.busy":"2022-04-06T18:08:53.607991Z","iopub.status.idle":"2022-04-06T18:08:53.613224Z","shell.execute_reply":"2022-04-06T18:08:53.612569Z","shell.execute_reply.started":"2022-04-06T17:59:01.682365Z"},"papermill":{"duration":0.027216,"end_time":"2022-04-06T18:08:53.613397","exception":false,"start_time":"2022-04-06T18:08:53.586181","status":"completed"},"tags":[]},"outputs":[],"source":["rnn1 = RNN()\n","rnn2 = RNN()"]},{"cell_type":"code","execution_count":4,"id":"2b6ffd12","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:53.655894Z","iopub.status.busy":"2022-04-06T18:08:53.655167Z","iopub.status.idle":"2022-04-06T18:08:53.657056Z","shell.execute_reply":"2022-04-06T18:08:53.657721Z","shell.execute_reply.started":"2022-04-06T17:59:01.693358Z"},"papermill":{"duration":0.025859,"end_time":"2022-04-06T18:08:53.65791","exception":false,"start_time":"2022-04-06T18:08:53.632051","status":"completed"},"tags":[]},"outputs":[],"source":["y1 = rnn1.step(x)\n","y = rnn2.step(y1)"]},{"cell_type":"markdown","id":"58374cca","metadata":{"papermill":{"duration":0.018188,"end_time":"2022-04-06T18:08:53.694955","exception":false,"start_time":"2022-04-06T18:08:53.676767","status":"completed"},"tags":[]},"source":["In other words we have two separate RNNs: One RNN is receiving the input vectors and the second RNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care - it’s all just vectors coming in and going out, and some gradients flowing through each module during backpropagation.\n","\n","Getting fancy. I’d like to briefly mention that in practice most of us use a slightly different formulation than what I presented above called a Long Short-Term Memory (LSTM) network. The LSTM is a particular type of recurrent network that works slightly better in practice, owing to its more powerful update equation and some appealing backpropagation dynamics. I won’t go into details, but everything I’ve said about RNNs stays exactly the same, except the mathematical form for computing the update (the line self.h = ... ) gets a little more complicated. From here on I will use the terms “RNN/LSTM” interchangeably but all experiments in this post use an LSTM."]},{"cell_type":"markdown","id":"6d435023","metadata":{"papermill":{"duration":0.018015,"end_time":"2022-04-06T18:08:53.731522","exception":false,"start_time":"2022-04-06T18:08:53.713507","status":"completed"},"tags":[]},"source":["## Character-Level Language Models\n","\n","Okay, so we have an idea about what RNNs are, why they are super exciting, and how they work. We’ll now ground this in a fun application: We’ll train RNN character-level language models. That is, we’ll give the RNN a huge chunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.\n","\n","As a working example, suppose we only had a vocabulary of four possible letters “helo”, and wanted to train an RNN on the training sequence “hello”. This training sequence is in fact a source of 4 separate training examples: 1. The probability of “e” should be likely given the context of “h”, 2. “l” should be likely in the context of “he”, 3. “l” should also be likely given the context of “hel”, and finally 4. “o” should be likely given the context of “hell”.\n","\n","Concretely, we will encode each character into a vector using 1-of-k encoding (i.e. all zero except for a single one at the index of the character in the vocabulary), and feed them into the RNN one at a time with the step function. We will then observe a sequence of 4-dimensional output vectors (one dimension per character), which we interpret as the confidence the RNN currently assigns to each character coming next in the sequence. Here’s a diagram:\n","\n","![](http://karpathy.github.io/assets/rnn/charseq.jpeg)"]},{"cell_type":"markdown","id":"3d7e803f","metadata":{"papermill":{"duration":0.018386,"end_time":"2022-04-06T18:08:53.768315","exception":false,"start_time":"2022-04-06T18:08:53.749929","status":"completed"},"tags":[]},"source":["An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and red numbers to be low."]},{"cell_type":"markdown","id":"cebde30a","metadata":{"papermill":{"duration":0.017833,"end_time":"2022-04-06T18:08:53.804335","exception":false,"start_time":"2022-04-06T18:08:53.786502","status":"completed"},"tags":[]},"source":["# Andrej Karpathy min char rnn"]},{"cell_type":"markdown","id":"a20b003a","metadata":{"papermill":{"duration":0.017852,"end_time":"2022-04-06T18:08:53.842216","exception":false,"start_time":"2022-04-06T18:08:53.824364","status":"completed"},"tags":[]},"source":["https://gist.github.com/karpathy/d4dee566867f8291f086"]},{"cell_type":"code","execution_count":5,"id":"77b2ccc1","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:53.887936Z","iopub.status.busy":"2022-04-06T18:08:53.881896Z","iopub.status.idle":"2022-04-06T18:08:54.645828Z","shell.execute_reply":"2022-04-06T18:08:54.645214Z","shell.execute_reply.started":"2022-04-06T17:59:01.709719Z"},"papermill":{"duration":0.785356,"end_time":"2022-04-06T18:08:54.645997","exception":false,"start_time":"2022-04-06T18:08:53.860641","status":"completed"},"tags":[]},"outputs":[],"source":["!echo 'An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and red numbers to be low.' > input.txt"]},{"cell_type":"code","execution_count":6,"id":"52d056c4","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:54.687639Z","iopub.status.busy":"2022-04-06T18:08:54.6868Z","iopub.status.idle":"2022-04-06T18:08:55.45228Z","shell.execute_reply":"2022-04-06T18:08:55.452872Z","shell.execute_reply.started":"2022-04-06T17:59:02.482482Z"},"papermill":{"duration":0.78812,"end_time":"2022-04-06T18:08:55.453093","exception":false,"start_time":"2022-04-06T18:08:54.664973","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and red numbers to be low.\r\n"]}],"source":["!cat input.txt"]},{"cell_type":"code","execution_count":7,"id":"7e6a9c9b","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:55.495948Z","iopub.status.busy":"2022-04-06T18:08:55.494812Z","iopub.status.idle":"2022-04-06T18:08:55.507912Z","shell.execute_reply":"2022-04-06T18:08:55.507235Z","shell.execute_reply.started":"2022-04-06T17:59:03.246257Z"},"papermill":{"duration":0.035653,"end_time":"2022-04-06T18:08:55.508108","exception":false,"start_time":"2022-04-06T18:08:55.472455","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["data has 375 characters, 38 unique.\n"]}],"source":["\"\"\"\n","Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n","BSD License\n","\"\"\"\n","import numpy as np\n","\n","# data I/O\n","data = open('input.txt', 'r').read() # should be simple plain text file\n","chars = list(set(data))\n","data_size, vocab_size = len(data), len(chars)\n","print('data has %d characters, %d unique.' % (data_size, vocab_size))\n","char_to_ix = { ch:i for i,ch in enumerate(chars) }\n","ix_to_char = { i:ch for i,ch in enumerate(chars) }\n","\n","# hyperparameters\n","hidden_size = 100 # size of hidden layer of neurons\n","seq_length = 25 # number of steps to unroll the RNN for\n","learning_rate = 1e-1\n","\n","# model parameters\n","Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n","Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n","Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n","bh = np.zeros((hidden_size, 1)) # hidden bias\n","by = np.zeros((vocab_size, 1)) # output bias\n"]},{"cell_type":"code","execution_count":8,"id":"154bbdfa","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:55.563392Z","iopub.status.busy":"2022-04-06T18:08:55.562528Z","iopub.status.idle":"2022-04-06T18:08:55.564485Z","shell.execute_reply":"2022-04-06T18:08:55.565169Z","shell.execute_reply.started":"2022-04-06T17:59:03.261806Z"},"papermill":{"duration":0.038009,"end_time":"2022-04-06T18:08:55.565363","exception":false,"start_time":"2022-04-06T18:08:55.527354","status":"completed"},"tags":[]},"outputs":[],"source":["def lossFun(inputs, targets, hprev):\n","    xs, hs, ys, ps = {}, {} , {} , {}\n","    hs[-1] = np.copy(hprev)\n","    loss = 0\n","    \n","    for t in range(len(inputs)):\n","        xs[t] = np.zeros((vocab_size, 1))\n","        xs[t][inputs[t]] = 1\n","        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n","        ys[t] = np.dot(Why, hs[t]) + by\n","        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n","        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n","    \n","    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n","    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n","    dhnext = np.zeros_like(hs[0])\n","    \n","    for t in reversed(range(len(inputs))):\n","        dy = np.copy(ps[t])\n","        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n","        dWhy += np.dot(dy, hs[t].T)\n","        dby += dy\n","        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n","        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n","        dbh += dhraw\n","        dWxh += np.dot(dhraw, xs[t].T)\n","        dWhh += np.dot(dhraw, hs[t-1].T)\n","        dhnext = np.dot(Whh.T, dhraw)\n","    \n","    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n","        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n","    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"]},{"cell_type":"code","execution_count":9,"id":"966cc5d3","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:55.607148Z","iopub.status.busy":"2022-04-06T18:08:55.606382Z","iopub.status.idle":"2022-04-06T18:08:55.613735Z","shell.execute_reply":"2022-04-06T18:08:55.614377Z","shell.execute_reply.started":"2022-04-06T17:59:03.279397Z"},"papermill":{"duration":0.030144,"end_time":"2022-04-06T18:08:55.614587","exception":false,"start_time":"2022-04-06T18:08:55.584443","status":"completed"},"tags":[]},"outputs":[],"source":["def sample(h, seed_ix, n):\n","    x = np.zeros((vocab_size, 1))\n","    x[seed_ix] = 1\n","    ixes = []\n","    for t in range(n):\n","        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n","        y = np.dot(Why, h) + by\n","        p = np.exp(y) / np.sum(np.exp(y))\n","        ix = np.random.choice(range(vocab_size), p=p.ravel())\n","        x = np.zeros((vocab_size, 1))\n","        x[ix] = 1\n","        ixes.append(ix)\n","    return ixes"]},{"cell_type":"code","execution_count":10,"id":"8e6a497a","metadata":{"execution":{"iopub.execute_input":"2022-04-06T18:08:55.658259Z","iopub.status.busy":"2022-04-06T18:08:55.65748Z","iopub.status.idle":"2022-04-06T18:11:41.514426Z","shell.execute_reply":"2022-04-06T18:11:41.515553Z","shell.execute_reply.started":"2022-04-06T17:59:03.298884Z"},"papermill":{"duration":165.881458,"end_time":"2022-04-06T18:11:41.515956","exception":false,"start_time":"2022-04-06T18:08:55.634498","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["----\n"," mfe\n","f\n","\"\"iyroNx-.yAg\"e4 we-paA\"pd3\"4lnlcNW( lRd\n","WRtyrr\n","fl\"Tytp3 f-),c;lweep\n","\".tco\"lmbg upo,xrTWx)adi lTp;;Tw-edtpA3s-N4 \n","wdh\n","Alvoe\"ahaoTNwdgT(Tlaidr\"R\"\"Thlhf;t Rm  w3,AAo)3tnovmrlbR(w.\n","-yAyTvryv;\"W,-py \n","----\n","iter 0, loss: 90.939649\n","----\n"," 3hTtsubdheo mbtuu)tdnbt whddwe  soutsgutp4ennautemnd andsus  le sas spucNatd Ror.Runoy s )e s3rismo;Rou  it.eltpaxaRsbguslxtWxurpyefisRh uon 4 hpstoe)tddx-Rp3trpsuex\"staaxhilery urtu;npauhpaun3benaot  \n","----\n","iter 100, loss: 91.509241\n","----\n"," ttsnr;ru(es(-n sginiintcetsiTo )hdtsiosa)t ndwa ce ac d Nnuir ensoi en  pin tt  go ty-W mt 4ntf4yo borsuuesidsthoh 4y.wb4rwnit\"Th3sa3-es3o bg mpiepd yl tnns wRuRotrstsn,oArh;rr)nwl,rna- ia,  nye nfens \n","----\n","iter 200, loss: 90.183597\n","----\n"," twennastonstidry\"s RlN  futiol p,\n"," iors sR ii4 ar  ghryorg)avrtimu,t . xne ter  oas nre he \"hbu pe)ii  ibns yert cthotsitheiasnis\"n (n  RRr nnshir nnduche Wpb tal,t\"m o4r iaeTiar ,bbi orwirn wnr fgn   \n","----\n","iter 300, loss: 88.131993\n","----\n"," xe ca\n","wd om(3rgne gneit tft4Woyaiad vyiitnrad g)(eeas pba.pN \"hRNo R daiu ri- v 4.rta ar(fows ohe  thava(t, amectors ylcoor t fiagna4\".u -o annrieeas ide tsddehe s ae viN 4)dsd Tdhetsd,veun faiiont ,i \n","----\n","iter 400, loss: 85.423067\n","----\n"," oaNns totb,se ces. lf Rern etthena. Thi aswd ctigndes l\"Nthlu n,w t fhcwaheneaxos blehens loxt. actes 3nutbRut eol  wite axps (p(vi futas digre smendt laybrath e n sris t eRtheouclm lulahe g-p nideuth \n","----\n","iter 500, loss: 82.282928\n","----\n"," e Tf eNt,rR he Nnye .ora,oy ime te tor\"octariss thoRer wfectedes oxand vthe thar haRens 3rxacor bonirds the The d wiroxl,yaeerutre arye ro. ut ame a ocithe ss fan, a. ares fo)g)-ugWd ide\" ara,sr ahila \n","----\n","iter 600, loss: 78.901068\n","----\n"," n ede his W4Nenr Nsigovacohar\" atnonlhal Nhe oNhi ar Nlas tithene3 (vocR4-(Te N(comi onnie\" rofig inticte bayen ,o,Nt Rhe che Tu,e ctext laaN ane fh lhignias \"he \"t nn th RT 4he uter3 conbesi tN anxoa \n","----\n","iter 700, loss: 75.298528\n","----\n"," las thors)rnt ther eharutiron rommddetederne  nyenenrs ow(whpN laonyebe RN thi nNN wast wacehp halens tharunnlut lenplndnt talosal  arehalens)n Tber (byeeu sitnpt d R waigns nAmbut,l,sWen ate winh w e \n","----\n","iter 800, loss: 71.578103\n","----\n"," hion inayelaN as ithe s 3 f hitighsis f tivathut lane guroxt). To cRNiunits f RNwo,); Wivetayis ehewen laolTar cthers lard andfpd th pam3 une un ia(ghil, fithele cN) wnitaus une une anrd hitanyeuu)ps  \n","----\n","iter 900, loss: 67.797377\n","----\n"," alf The aes Thees Thacontds ont na tvg3 ctbuia tame f RNce oasrsoo(tluro\"po ayees wovar fig ins the axwam tiocthxlens (ntidia t lgrd 4-dimd asrixpN and ante te RNN phd abers tavalann iagrfons ingns ut \n","----\n","iter 1000, loss: 63.939933\n","----\n"," ut cberd \"heN We,l\"igis focibularyiyits (utiins ). The RNl\"hoNs (n 4b. ahi anurs lnit the she RNNesd namenrt onar v3tns thawars the gr bb. eingrs ln icontisrt layen \"ons utits (ne RN 3 a\"he anteut las \n","----\n","iter 1100, loss: 60.234950\n","----\n"," N ayene ccb\"ia on(his nne RN N -higns the RNN ayerreut. The ghe m cithin, antbe t dee dic ayend f 3putt tidig(de gnputbus dhe t whir cahehe l wam c. che, (i inend \"hellxs. Th ioeRlas is cha4yignut cay \n","----\n","iter 1200, loss: 56.490864\n","----\n"," e cthicayd onputs fed euN hary onumbir cay ignlayputin (tp\"he awahondes (natR hane 4,3pm, he 3hend abnshons the -ext. m 3 cter (adpe nawttrd -ayers foxtit ca\"me vamis ctpnd iomouraym ains the non )om  \n","----\n","iter 1300, loss: 52.789884\n","----\n"," n axamplen wars confidenghutn RNahie, ashe  ayeRe r t dhituu(ncthealens for thehe s enddenc bica)\" RN\"he wnitoutid gs Thd as if tis the ,eut pamer thin ndmb,eacthxe canr \" foctpr\"fed anns the RNN a pi \n","----\n","iter 1400, loss: 49.281001\n","----\n"," layem co\"ss en the  figns fors). Thi anputeur t lid Wiits )h mhe layer the  nuth beder ws confers the RNN onactpr utpe layer conshe acticas fhe ol,onfirs thele nuthe e RNe rd onder (vocabthararaons (n \n","----\n","iter 1500, loss: 45.978735\n","----\n"," his ins the wombers the d e cof d es thirput. The  NNR  hivpt wi hangrs for en\"r,on uis to fer chitrdia th 4-demens tndi illN who RNN waiis the (i N onfers the 3 ouips inputaxa onand ai andef (n h,ras \n","----\n","iter 1600, loss: 42.685128\n","----\n"," s tis no\"); Wm 3 RNN  ayame actirlenute laconfid cons the RNN nsar iaye ande cay 4-dim, aacinicof f asyis inshe activacis the d tilo\"on hAlonayohurons. m)r ame nutirntout. \"),iasdis tivarssigrs for th \n","----\n","iter 1700, loss: 39.810402\n","----\n"," nput the andens for (npndds ins cono fheRlwo; Witput lafenolunsdens insionass wh,e,l,o\" fr input paye unon layer onsis lw ay ionras the RNN arth fars whe green numbitacters, ane she autp fid lanan haw \n","----\n","iter 1800, loss: 36.822050\n","----\n"," N assigns fo a. The arherafe ws thengnidees ther cheranss nyehe thend anend the uteacter (vocahi and os fed the outp aas wont sions th,en bolbeumions cof ; Withe ners). wad is out the a fers inp3 \"i;  \n","----\n","iter 1900, loss: 33.983547\n","----\n"," , ais ut folan and tayer en(vocabrens fecns fors). (vhen ell). a3d 4oniton and abyen \"rendciayers itin lanth, Then the and a)n and ais lontior bhe l; mhiteuRNNl, isrsutn rary is fh, asin us whe claren \n","----\n","iter 2000, loss: 31.394494\n","----\n"," s example RNN actidad the chawand the gcte 4-debe the output,lf tidid t yers,tm confid layer onfid inandoutevnteins the ar anput layer con a d avacands no\"); We want then the RNN is ind oura as \"harsr \n","----\n","iter 2100, loss: 29.393820\n","----\n"," con \"he ut end \"hevntainsis in iant onth 4(diis when layer ons). Tharat f 4ide wasr charonteutn Tne guthe RNN ass the RNN4enghe o(tputhe RNN is f dif t lidenl). This dia cinput layer chiracter This in \n","----\n","iter 2200, loss: 27.054064\n","----\n"," hi (ners \"hy inRNN Ta,t4-dis lactocteren charns iounons for the wame pacticannons inens f Tiensinuryenshenuthewns ia talen the liss iopuro\"ho aicin to beder of 3 units (nenye cass the 3 niconfurs the  \n","----\n","iter 2300, loss: 24.874242\n","----\n"," ass wh,e\" w tiddee RNNe ghe The output layer ctp\"x,iactirnt the onters \"hell\" amer of 3 champlen wnput onan)f 3 units (neurons). This diagram s ont wayard pam(de cofbe green nut RN ivenacaprward pasye \n","----\n","iter 2400, loss: 22.859837\n","----\n"," nput. The output las yars whe ghe greenuut thd and the RNN is fed the cher cayer orsiss (ne nero. The outputnlas input and output layiinunutin layer \"he Tactput facterd pass \"he activations in the for \n","----\n","iter 2500, loss: 21.006930\n","----\n"," r wiont th iace\"N asarent RNNe, and asdes (uendis confidences the RNN assigns confidences thee RNNep is wiews the RNNionth of 3 unitxl input and output layers, ant areroutpe eer t l4yers, and a hicann \n","----\n","iter 2600, loss: 19.293018\n","----\n"," e une r the gucobulTro outpud \"bersh, a 3 cis fides widdens nuumbulatpun,lxs the forward pass \"he output layens ona4confidences the RNN assigns for the nexteurs hamen inprss f diows inen the RNN is fe \n","----\n","iter 2700, loss: 17.712551\n","----\n"," n example RNN with 4-dimens toft3 characters \"he forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains thele theecthend tactor the RNN as input thalactinn (nut for \n","----\n","iter 2800, loss: 16.253004\n","----\n"," layers, and a hidde s the RNN assigns uthl fors forssionals onte cahend the onits (neinext the (nen hpt f coN hiagrss no o; We inl(noutput layer contains confiden othe RNN we,is the next chid a theh o \n","----\n","iter 2900, loss: 14.915663\n","----\n"," hisl tacher number consigns confidences the RNN assigns ah; authe atee l,o\"); Wr chavacand output rary bas and tar contains the RNN assigns for the next layer contains confidente lamen the forward pas \n","----\n","iter 3000, loss: 13.687798\n","----\n"," ass \"he the utit chishit layer tas inand on The unee cher the (n ohons tade wa)ns utalN anctasne layer of 3 units 3ns (neurons). This diagramautacte and (oonayionuut forld ontioncen nWer ohilnteins (n \n","----\n","iter 3100, loss: 12.559585\n","----\n"," nput \"hellagrsons fegn layer  ayer capar harward pass the output layen ts when the RNNut las onyer cthe nnt 4naigh lonans fed the the actithe output layer contains conTbume farwes (ns 3 unitr (neuconb \n","----\n","iter 3200, loss: 12.148722\n","----\n"," N assiins foutios (neurons opTyers, and a hhe RNN nivations fed th, amdiasd the onhellcofs)ss buthe w am ber chis dithe artender chirf waarasionat ind )bulo\" aye,,eus)ns the he l fv;llayer contains co \n","----\n","iter 3300, loss: 11.252289\n","----\n"," ,e,l,o\"); We want the green numWs fhe green numbers to Tas  anth of 3 units (nauto hacan nt en thellnp\"), high and outputacis \"n,e,l,iant the green noutiss (ne nem hh 4-dimensional input and outs the  \n","----\n","iter 3400, loss: 10.357556\n","----\n"," n example wnen contand thexd en (nsinput. The output layers the fer to ou puthe RNN assigns cofld sidhee wamen luhars fhe the \"haracters \"mell\" assighaws the activations in the dis,d,is it The far irs \n","----\n","iter 3500, loss: 9.528432\n","----\n"," layen . Whe gree layer of 3 units (neid fed andte ge 4-dim ons olaye o(ss f co shindxt chalar the next charact,ractergrs inarnthd andid andis coanutolconf hadies waln lanel the RN hirarnt the activati \n","----\n","iter 3600, loss: 8.766430\n","----\n"," his diagcovfin anadd han iont f cons). camhigr ahy gnput. Thenutoutaidsigns the RNN assigns fors). (wors onfids confidencas the RNN assigncoassis when the RNN is fed tivocabulan. ons camen the ffbensi \n","----\n","iter 3700, loss: 8.066846\n","----\n"," ns). Then numbers to be high put and output layers, and a hidden lay iars the RNN assigns for the next character RNN is fed the charactivatishenactivations in the forward pas)d and ber contains the RN \n","----\n","iter 3800, loss: 7.425442\n","----\n"," nput. The output layer contains confbes the RNN assigns for the next character (vocabulary is \"h,e,lassigns confidences the RNN assigns for the next chararsdis fors).s nyer of d ,inen bum, haramwnllan \n","----\n","iter 3900, loss: 6.837372\n","----\n"," N anthe output layer (ontaid conteddxt lanen ctp\"s,eagram shows the activations in the forward pass when the RNN is fed the characterssnwis \"h,e,l,o\"); We want the cthe RNN is ahe green numbers to be  \n","----\n","iter 4000, loss: 6.298469\n","----\n"," ,e,l,o\"))shialas on for the next character (vocabulary is \"h,e,l,o\"); Wit layers, and a hidden layer of 3 units (neurons). This diagram shows thd athalaNN on hall\" a hivhcabulary is \"h, a tictne fars  \n","----\n","iter 4100, loss: 5.804797\n","----\n"," n example RNN withe n hh anders tol4ydiyensh,e,l,o\"); hithe we lio\"); We want the green numbers to be high and outprs ind aheictne (ayer o\" amend the f 3 units the next character (vocabulary is \"h,e,l \n","----\n","iter 4200, loss: 5.352222\n","----\n"," layers, and a hidden layen the fsr onyers so o; We want the f focoutput fid anrocter campl, asd the characters \"hell\" amen the forward pass when the RNN tons walanard pass when tactainse nars waceacte \n","----\n","iter 4300, loss: 4.938561\n","----\n"," his dof hid inpl onber cthN andiins (nthe \"hahal of (ne nextidsnons (naontxs chivacaids confideuces anances the RNN assigns forsddsional input and output layers, and a hidden layer of 3 units (neurons \n","----\n","iter 4400, loss: 4.559862\n","----\n"," ass when the RNN is feen the RNN is fed ehen the RNN is fed the cher cayel \"vato avumeitsl onThicpacandes (ie next character (vocabulary is \"h,e,l,o\"); We want the athe RNN is \"he activations in the f \n","----\n","iter 4500, loss: 4.213689\n","----\n"," nput. The outpolutwarn is un the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (fpxt character (vocabula \n","----\n","iter 4600, loss: 3.896626\n","----\n"," N assigns for the next layet ctput labensicns for the next charartits nn abularyer o\"); We want the grelRNN with 4-dimensional input and output fad the when the RNN is ont the acth 4-dimenshinal(nut l \n","----\n","iter 4700, loss: 3.606490\n","----\n"," , a N cher cfer the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high ons mburat onde, (neurons). This diagram shows the activations in thellas the forgns lonx). This diag \n","----\n","iter 4800, loss: 3.341427\n","----\n"," n example RNN with 4-dimensional input lRNN assiont 3is inalen layer of 3 units (neurons). This diagram shows the activations in the.dxt cayen layen oxayomx). Tblladdiss asd agram shut enllof 3 che ar \n","----\n","iter 4900, loss: 3.098394\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in inayerss fhd a hidden layer of 3 units (ions fe onn). This diagram shows the activations wnen thexd pathe (vocabu \n","----\n","iter 5000, loss: 2.876387\n","----\n"," his diactis chal inen hixauts wnen the RNN is fed the characters \"he forward pass \"he RNN with 4-dimensional \"ferss (neurons). This diagram shows the acte The RNN is fed the characters,ds fed (neurons \n","----\n","iter 5100, loss: 2.672973\n","----\n"," ass ghe output layer coarains confidences the RNN anais f did (nassionte the RNN is f RNN assigns for the s ihe chaifl ons). Thi and oins (neurand Wars wnen the and output layers,d,ifoctnrs 3 units (n \n","----\n","iter 5200, loss: 2.486560\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neu \n","----\n","iter 5300, loss: 2.315817\n","----\n"," N ayend the ctayer (fer tid ar ons layer contains confidences the RNN assigns for the next character (vocabuthe gteen hulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and  \n","----\n","iter 5400, loss: 2.159135\n","----\n"," ,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 uuras chaciers \"hell\" as input. The output layer contains confidences the RNN assigns for the next character ( \n","----\n","iter 5500, loss: 2.015721\n","----\n"," n example RNN with 4-dimensional input and output layers,ssional inpuulTh raconyers the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want andis The npxt wamen lend encand output ca \n","----\n","iter 5600, loss: 1.883817\n","----\n"," layeis, and as)is onant the green numbers to 4y pith 4-dimens confidences the RNN assigns fer the RNNdens cof 3 units (neurons). This diagram shows the acters,lass when ten and output layers, and a hi \n","----\n","iter 5700, loss: 1.763178\n","----\n"," his diagram shows the activararsut 4nd is wnen the and output layers, and a hidden layer of 3 units (neuroncthe nerbenptee cay ivpcabulary is output ena. the high and output layers, and a hiddend ayen \n","----\n","iter 5800, loss: 1.652294\n","----\n"," ass when the RNN is  ber concains confis layer \"he cortps for the next faron and hars the RNme wasdis 3outite RNN assigns for the next character (vocabulary is \"h,e,lT fignons it the forwardtions in t \n","----\n","iter 5900, loss: 1.550455\n","----\n"," nput)y yer contains confidenwas onal on RNme want the d ther lamen lumhigncand pacan the forward pass when the RNN igrs the RNN assigns for the fer to be high and output layers, and a hidden layer of  \n","----\n","iter 6000, loss: 1.456770\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbe. Tne ss when the RNN is fed the characters \"hell\" as input. The oute war and baractputit cay and bample \"Re f 3 unit \n","----\n","iter 6100, loss: 1.370530\n","----\n"," ,e,l,o\"); Wed aNN with 4-diben the RNN is fed the characters \"hell\" as input. The outa and output layers, and a hidden layer of 3 units (neurons). This diagractars tollashign and asput,eons fed the ch \n","----\n","iter 6200, loss: 1.291351\n","----\n"," n example RNN with 4-dimentidealT hid amen ayerss on nolgns the RNN ass nis focis chonon nances the RNN assigns for the next character chpant fed the characters \"hell\" as input. The outpons when the R \n","----\n","iter 6300, loss: 1.218172\n","----\n"," f d Whel  arpe ald pass when the RNN is fed the charactergrssionsigns for the RNN nyer of 3 units (nambula)3 The RNN is fed the characters \"helln (ueurons). This diagram shows the activations in the f \n","----\n","iter 6400, loss: 1.151092\n","----\n"," his for (nen thellwoutputo ); We want the charactitarss onathe ciart \"helR\"ciounits (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"helnn tod asai \n","----\n","iter 6500, loss: 1.089159\n","----\n"," ass whens the next character (w. This diagram shows the activations in the forwardd and a hidden layer easyer (neurons). This diagram shows the activations in the forward pass when the RNN is fed the  \n","----\n","iter 6600, loss: 1.032016\n","----\n"," nput. The output layer ctell)y bes tnpxt forward pass wnen the RNN is fed the characters \"hell\" as input. The ourer thNlencthis inatnn layer \"mbid (n\" Th wn the foctp\"); Ween conrains confidences the  \n","----\n","iter 6700, loss: 0.979197\n","----\n"," N assigns for the next character (vocabulary is fed the ciame de the layer of 3 units (neurons). This diagrp tame ces (neuroxs tagram shows inen the RNN is fe,ns (neurons). This diagram shows the acti \n","----\n","iter 6800, loss: 0.930315\n","----\n"," ,e,l,o\"); We want the output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and asis confidences the RNN assigns for  \n","----\n","iter 6900, loss: 0.885247\n","----\n"," n example RNN with 4-dimensional input and outpct\"ra yer pass whe aN en numbers to be high and output layergrssinuctar (vocabulary is \"h,e,l,o\"); We want the output layer contains confidences the RNN  \n","----\n","iter 7000, loss: 0.843295\n","----\n"," layers, and a ay ivpantis foco; Ween layer of 3 units (ne nens). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains co \n","----\n","iter 7100, loss: 0.804721\n","----\n"," his diagram shows the activations in the forward pantis coawits wo ayers,ous. the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary i \n","----\n","iter 7200, loss: 0.768874\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The shioutitputpxt chalactid anuthe ater (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and o.oube ahen the RNN is fed the char \n","----\n","iter 7300, loss: 0.735594\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the gcters). This diagram shows the activations in itide,l,o\"); We want the guthe  \n","----\n","iter 7400, loss: 0.704632\n","----\n"," N assigns for the next character (voc(nput layer (vocabulary input \"hell\" ayer of 3 unend inun \"hellas thilnt itacters cayer (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output  \n","----\n","iter 7500, loss: 0.675770\n","----\n"," ,e,l,o\"); We wnitne numberse). This diagram shows the activations ontes (nen ons). This diass wbe RNNions in the forwavatput ehigh and output layers,o,lampl hal onfid ns ind aheign the forward pass wh \n","----\n","iter 7600, loss: 0.649017\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hiddenut \"waracter (vocabulaNr men and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the \n","----\n","iter 7700, loss: 0.623868\n","----\n"," la)f The RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the fer the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high \n","----\n","iter 7800, loss: 0.600660\n","----\n"," his deagram snors the green numbers to be high and output layers, and a hi anu onpantions in the forward pass when the RNN is fed the characters \"hell s,en number chinand ss ane s onanen tne sayerwame \n","----\n","iter 7900, loss: 0.578912\n","----\n"," ass when the RNN is rutis fortrss)o andis nout. We want the green numbers to be high and oiss con); We want the RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). \n","----\n","iter 8000, loss: 0.558561\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulamb taracterd \"hell\" as input. The output layer ctput layer contains confidences the RNN assigns for the next \n","----\n","iter 8100, loss: 0.539488\n","----\n"," N assions (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The outputpchiontains confidences the RNN assigns for the next character \n","----\n","iter 8200, loss: 0.521565\n","----\n"," ,e,l,o\"); hh-dite con asnputinons in the forward pass when the RNN is fed the characters \"hell\" f Weut and ispons ins ont vatithe grees (neuxa (n. be wigh 4-dis waRNa bensions the next onyers focis fi \n","----\n","iter 8300, loss: 0.504875\n","----\n"," n example RNN with 4-dimensionalins dishe fhe utit l3 ch,ian outyeas ons). chis diaws it the farwpayer contains confiden handes wolayers, and a hidden layer of 3 units (neurons). This diagram shows th \n","----\n","iter 8400, loss: 0.489013\n","----\n"," layers, and theictnt chis diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The green numbers to be high and output layers, and a hidden layer of 3  \n","----\n","iter 8500, loss: 0.474350\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. Tner cayen anany bamplen the RNN is fed the chars conv; We want the green numbers to be high a \n","----\n","iter 8600, loss: 0.460489\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences chalarshutons ine anits (neuronss We wan nt f r esd enconspue War a lions in the forward pass whe outp\"has \n","----\n","iter 8700, loss: 0.447413\n","----\n"," nput. The output layer cowtains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers the RNN ahars onte 4bulary is ons on afegns the RNN assigns for  \n","----\n","iter 8800, loss: 0.435050\n","----\n"," N nicanr (vorayers, and a hidden layer of 3 units for the next character (vocabulary is \"h,e,l,o\"); We want the green num); chasal ntideactivations in the forward pass when the RNN is fed the ghe the  \n","----\n","iter 8900, loss: 0.423353\n","----\n"," ,e,l,o\"); Widealuors \"hitp\"); We want the green numbers to be high and output layers, and a hi and ons input. The output layer contains confidences the RNN assigns for the next character (vocabulary i \n","----\n","iter 9000, loss: 0.412420\n","----\n"," n example RNN with 4-dimensional input and output layers, and agharssishe charactpute The output layer containsedes for the next character (vocabulard is \"h,e,l,o\"); We want the green numbers to be hi \n","----\n","iter 9100, loss: 0.401922\n","----\n"," layi)git canthen cahe re and hars the RNN assigns for the ners character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neurons). Thi \n","----\n","iter 9200, loss: 0.392142\n","----\n"," his diagram shows the ars wayen nuryers to be high and output layers, and a hidden layer of 3 chas, RNN assigns confors thawar (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and outpu \n","----\n","iter 9300, loss: 0.382836\n","----\n"," ass when the RNN is fed the chacan ns confidences the RNlay m); characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We w \n","----\n","iter 9400, loss: 0.373929\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and out ulora)ns the RNme l\" 3 units (neurons). This  \n","----\n","iter 9500, loss: 0.365493\n","----\n"," N assigns the RNN ass nis bhe cen the next character (vocabulary ts ane green numbers to be high and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the f \n","----\n","iter 9600, loss: 0.357388\n","----\n"," ,e,l,o\"); We want the green numbers ). The  ame de  hm be RNN with 4-dimensional input and output layers, and a hidden contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l \n","----\n","iter 9700, loss: 0.349755\n","----\n"," n exasarward puts \"he activations in the forward pass when the RNN is fed the characters \"hell\" ashignc(vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden l \n","----\n","iter 9800, loss: 0.342335\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass wnen the RNN is fed the chamal of hide wasd when the layer of 3 units (neurons). This diagram sh \n","----\n","iter 9900, loss: 0.335394\n","----\n"," hisndiagram shows the activations in the forward pass when the RNN is fed the cher cayel \"helnn tolbeo\"); We want in the forward pass when the RNN is fed the characters \"hell\" as input. The output lay \n","----\n","iter 10000, loss: 0.328770\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and \n","----\n","iter 10100, loss: 0.322385\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hiddens fo\" uh,in r the next \n","----\n","iter 10200, loss: 0.316236\n","----\n"," N assigns for the next chad Wirons). This diagram shows the output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and \n","----\n","iter 10300, loss: 0.310345\n","----\n"," ,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the chawant the sh \n","----\n","iter 10400, loss: 0.304731\n","----\n"," n example RNe cthe RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next chararas onde layer of 3 units (nen,en numbers to be high and output la \n","----\n","iter 10500, loss: 0.299210\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"ferssionarward pass when the RNN is fed the characters \"henp \n","----\n","iter 10600, loss: 0.294102\n","----\n"," his diagram shows thd activatilas haractplane green numbers to be high and ); ontis diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output lay \n","----\n","iter 10700, loss: 0.289099\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN antid ase ons in the forward pass when the RNN is fed the characters \"hell\" as input. The output l \n","----\n","iter 10800, loss: 0.284288\n","----\n"," gr The RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and outpu \n","----\n","iter 10900, loss: 0.279679\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); Wel ane conteutors). This diagram shows the activations in the forward pass when the RNN is fed euronss haconcnyer of 3 units (neurons). Thi \n","----\n","iter 11000, loss: 0.275127\n","----\n"," ,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neurons). This diagrarssrwss wion alas the next character (vocabulary is \"h,e,l,o\"); We want the green  \n","----\n","iter 11100, loss: 0.270795\n","----\n"," n example RNN with 4-dimensnonans sh,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neutout. Wegns ineuconfidences the nerl,o\"); Wid alN agrerss ont f  \n","----\n","iter 11200, loss: 0.266591\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in \"he forward pass woen the RNN is fed the characters \"hell\" as input. The output layer contains confid wt f dde)ge \n","----\n","iter 11300, loss: 0.262688\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confiss (neurons). This diagram shows the activations in the forward \n","----\n","iter 11400, loss: 0.258773\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to (f. 4-dimen \n","----\n","iter 11500, loss: 0.254947\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neu \n","----\n","iter 11600, loss: 0.251285\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the f \n","----\n","iter 11700, loss: 0.247877\n","----\n"," ,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neute\" as input. The output layer contains confidences the RNN assigns fers \"hell\" as ind onputis the T \n","----\n","iter 11800, loss: 0.244475\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neuronf;r cvp\"e,l\");oW4-dencos the ne the RNN is fed the characters \"hell\" as input. The output layer contains  \n","----\n","iter 11900, loss: 0.241005\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed units (neurons). This diagram shows the activations in the forward pass when \n","----\n","iter 12000, loss: 0.237899\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the charact radis the RNN asdit the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The ou \n","----\n","iter 12100, loss: 0.234795\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and \n","----\n","iter 12200, loss: 0.231734\n","----\n"," nput. The output layer contains confir ctivations in the forward pass when the RNN is fed the characters \"hellut \"haracter (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and ). Whe sh \n","----\n","iter 12300, loss: 0.228773\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the f \n","----\n","iter 12400, loss: 0.225850\n","----\n"," ,e,l,o\"); We want the green numbers ty ber cantains con(lo\"s chigh ayen (nenr helnoutput layer cayen avatput layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\") \n","----\n","iter 12500, loss: 0.223067\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagrarfors wo\"h, ale chahal nant it the forward pass when the RNN is fed the characters \"hell\"  \n","----\n","iter 12600, loss: 0.220219\n","----\n"," layers, and a hidden layer of 3 units (neunput. The output layer contains confidences the RNN assigns f;N assigns for the next chard outs to warsigns nnit layer contains confidences the RNr nssh, acti \n","----\n","iter 12700, loss: 0.217606\n","----\n"," his diagram shons when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers  \n","----\n","iter 12800, loss: 0.215004\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and \n","----\n","iter 12900, loss: 0.212466\n","----\n"," nput. The output layer contains confiden acas on,e ca\"ma pambr ovhvad ahe ghen the RNN is fe wc numbers to be high and output layers, and a 3ir the next character (vocabulary is \"hawagram shows the ac \n","----\n","iter 13000, loss: 0.209970\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the f \n","----\n","iter 13100, loss: 0.207487\n","----\n"," ,e,l,o\"); We want the green numbers to be higicanput. Ther cayen ard)put. The octilations in the forward pass when the RN hard bensiceRNN with 4-dimen iontpits wamd mhe l\" aors, and a hidden layer of  \n","----\n","iter 13200, loss: 0.205131\n","----\n"," n example RNN with 4-dimensional inpuulanpute nout uayer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is \"reen numbers to be high and output layers, and a  \n","----\n","iter 13300, loss: 0.202702\n","----\n"," lN ons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hello tivations in the forward pass when the RNN is fed the characters \"hell\" as input. The output l \n","----\n","iter 13400, loss: 0.200470\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the characters \"he forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidengh layer \n","----\n","iter 13500, loss: 0.198242\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The outh layed ame forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for th \n","----\n","iter 13600, loss: 0.196062\n","----\n"," nput. The ontput layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neu \n","----\n","iter 13700, loss: 0.193910\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); We want the charactput the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN an \n","----\n","iter 13800, loss: 0.191765\n","----\n"," ,e,l,o\"); We want and aheidden layer of 3 units (neurons). This diagram shows the activations in the forward pasnens \"haracter (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and outpu \n","----\n","iter 13900, loss: 0.189724\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hidden onput. The output layer contains confidences the RNN assigns for the next character (vocabulary ialayer contains confidences the  \n","----\n","iter 14000, loss: 0.187615\n","----\n"," layers, and axeideenuutions in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the fer the next character (vocabulary is  \n","----\n","iter 14100, loss: 0.185677\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h \n","----\n","iter 14200, loss: 0.183746\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and \n","----\n","iter 14300, loss: 0.181849\n","----\n"," nput. The output layers toeber chinand bample RNN with 4-did iagroxs cayer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high a \n","----\n","iter 14400, loss: 0.179975\n","----\n"," N assigns the RNN assighs for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neurons). This diagram shows the acti \n","----\n","iter 14500, loss: 0.178108\n","----\n"," ,e,l,o\"); We want the green numbers to be high and output layers, and a the RNe f r 3 uurors). This diagram shows the activatpuws in the forward pass when the RNN is fed the characters \"hell\" as input \n","----\n","iter 14600, loss: 0.176329\n","----\n"," n example RNN  ith\"i,eactirntion taciarnt then the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We \n","----\n","iter 14700, loss: 0.174489\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the charactexs toet the green numbers to be high and output layers, and a hi \n","----\n","iter 14800, loss: 0.172800\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h \n","----\n","iter 14900, loss: 0.171135\n","----\n"," asrd ss Tnut and onpagrs input charput thend saN and anp gnilasiwnthun,tal,ithe RRNNr then thery is \"her anpm npuws wntighe actilantm npm (r way gnt ontburon ut and gry is ons wn ias thend sais the aN \n","----\n","iter 15000, loss: 1.815251\n","----\n"," n hilars,\"a RNN anols fed d tidden tenra(ns 3 W ar iay caput lanen thend e\" ayer on bumber cantiins dis on t p\" ayencpass whe RNN with 4-dis tpenivatis (news th, autirsiss tne next the foront4its (ns  \n","----\n","iter 15100, loss: 4.613866\n","----\n"," N assigns fociona s fed the RNN wiwhar on (neurons)\"he laid a hidden layer of 3 units (nponpmm. phe characters \"he forward pas lwiddencthe forward pass when thend en to ber cagrains confidences the RN \n","----\n","iter 15200, loss: 5.171917\n","----\n"," ,e,l,o\"); Wo sactirsens coaber cay igr ahis she fer the next char characters \"hell\"); We want the green numbers to be high and output layers, and a hidden the RNN is fed ahel cantains utratf hars the  \n","----\n","iter 15300, loss: 4.908615\n","----\n"," n example RNN with le haracter (vocabulary is fed the characters \"ryers for the next character (voratishe activatious iatpacter the next character (vocabulary is \"h,eg4wamr challcabulayerat the output \n","----\n","iter 15400, loss: 4.548017\n","----\n"," layers, and a hidden layer of 3 units (neurons). Thes diagram shut,e,l,o\"he char Rlam shogre os or pantes (neurand pame RNN wat\"cabulaid is the output layer contains the RNN assigns fer the next char  \n","----\n","iter 15500, loss: 4.195497\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for insis fidden of 3 units (naber conta \n","----\n","iter 15600, loss: 3.863901\n","----\n"," ass when the RNN is fed the characters \"hell\" as inpntiss didiners). This diagrarnassigns for the next charact rarwarden the RNN is fed the ontput layer contains confidences the RNN assigns for the ne \n","----\n","iter 15700, loss: 3.556414\n","----\n"," nput. The output lhell\" as inputs Then contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer \n","----\n","iter 15800, loss: 3.272684\n","----\n"," N assigns for the next charact her of 3 units (neurons). This diagram uns confidences the as enput uay indins the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green nyers  \n","----\n","iter 15900, loss: 3.012169\n","----\n"," ,e,l,o\"); We want the gree cingawat ins confidences the RNN assigns for the next charact. WeN aneignens foTnut layer of 3 units (neurons). This diagram shows the activations in the forward pass when t \n","----\n","iter 16000, loss: 2.773358\n","----\n"," n example RNN with 4-dimensional input and output la. We camed ons mbumal, This diagram shows the f RNN assigns for the next character (vocabulary is \"h,e,l,o\")o hell\" aher of 3 units (neurons). This  \n","----\n","iter 16100, loss: 2.554602\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the (vocabulary is \"h,e,l,o\"); We want chad is fed the characters input and output layers,  \n","----\n","iter 16200, loss: 2.354915\n","----\n"," his diagram shows the activations in the forward pass when the RNNl tions on as char palthaws the green numbers to )as igh and onyer of 3 units (neurons). This diagram shows the activations in the for \n","----\n","iter 16300, loss: 2.171784\n","----\n"," ass when the RNNNis ie wartidee activations in the forward pass when the RNN is fed the characters \"hell\" a hagh put and output layers talll\" nidaros fors,o\" 3 units (neurons). This diagram shows the  \n","----\n","iter 16400, loss: 2.004354\n","----\n"," nput. The output vharactarass when cthe RNN assigns forounsions the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and ghee the R \n","----\n","iter 16500, loss: 1.851028\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and e 3 rsits cof 3 units (neurons). This diagram shows the activations in the forwa \n","----\n","iter 16600, loss: 1.710932\n","----\n"," ,e,l,of 3 units (he nexts when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN n War the gree layer of 3 units (neurons). This diagram shows the activation \n","----\n","iter 16700, loss: 1.582926\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is \"h,ealcos ther on th 4-dis ian \n","----\n","iter 16800, loss: 1.465937\n","----\n"," layers, and a hid en layer of 3 units (neurons). This the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output la onf-dences the RNN assigns fo \n","----\n","iter 16900, loss: 1.359233\n","----\n"," his diagram sholayer of ; lhevrs, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer c \n","----\n","iter 17000, loss: 1.261767\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN ass the next character (vocaayer of 3 units (neurons). This diagram shows the activatishe alen 4-d \n","----\n","iter 17100, loss: 1.172778\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the greenden layer of 3 units fociassighe fer the next chararnthe next character ( \n","----\n","iter 17200, loss: 1.091317\n","----\n"," N assigns for the next character (volath hur layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer con \n","----\n","iter 17300, loss: 1.016877\n","----\n"," ,e,l,o\");)We, and a hidden layer of 3 units (neurons). This diagram sh beerl of 3 units (neurons). Thissd ame cidis inautput. Tber contains confidences the RNN assigns for the next character (vocabula \n","----\n","iter 17400, loss: 0.948877\n","----\n"," n example RNN with 4-dimensional inpul is \"),e, RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be RNN withN aser of 3 units (neurons). Thisides (neurons). T \n","----\n","iter 17500, loss: 0.886626\n","----\n"," layers, and a hidden layen of 3 units (neurons). This diagram shows the alayer of 3 units (neurons). This diagram shows the activations in the forwacatn\" ahe RoN ayer of 3 units (ne numr Whe l\" (n,inp \n","----\n","iter 17600, loss: 0.829805\n","----\n"," his diagram shows the activations in the coryer the next character (ontions the RNN assignut,ewis on ard pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences  \n","----\n","iter 17700, loss: 0.777804\n","----\n"," aws wacend pass wher Nnput. The output layer contains confidences the RNN assigns for the next character (vocabular, is \"h, alen layer of 3 units (neurons). This diagram shows the activations in the f \n","----\n","iter 17800, loss: 0.730222\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output lartm th R-ioncthell\" as is charsional inp \n","----\n","iter 17900, loss: 0.686568\n","----\n"," N assigns for the next char; is fed the characters \"hell\" as input. The output l\"is the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains co \n","----\n","iter 18000, loss: 0.646560\n","----\n"," ,e,l,o\"); We want the green numbers th 4-dir warsions in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next charact \n","----\n","iter 18100, loss: 0.609880\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hilanursiss rout. hhens iss confidences the RNN assigns for the next characasrshd asre ssr har RNN is fed the characters \"hell\" as input \n","----\n","iter 18200, loss: 0.576202\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RN \n","----\n","iter 18300, loss: 0.545403\n","----\n"," his diagram  hacterd \"he lame RNN with 4-dimensional input and output lay haracter (vocabulary is \"h,e,l,o\"); We want the green numbers tol4bs input. The output layer contains confidence (vntayeass lo \n","----\n","iter 18400, loss: 0.517111\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confid wars the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the gteers layer contains confid \n","----\n","iter 18500, loss: 0.491119\n","----\n"," nput. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 units (neu \n","----\n","iter 18600, loss: 0.467155\n","----\n"," m  input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forwalarnut lactput layer contains confidences the RNN assigns for the next character (vo \n","----\n","iter 18700, loss: 0.445083\n","----\n"," ,e,l,o\"); We want the green numbers to be he las iant the activations in the forward pass wher contains confidencas tha4-d ifp\" for the next character (vocabulary is \"h,e,l,o\"); We want the green numb \n","----\n","iter 18800, loss: 0.424742\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as i \n","----\n","iter 18900, loss: 0.405947\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidencen np\"be  \n","----\n","iter 19000, loss: 0.388687\n","----\n"," his diagram shows the activations in the forward pass when layer onuto be RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in \n","----\n","iter 19100, loss: 0.372727\n","----\n"," ass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and \n","----\n","iter 19200, loss: 0.357967\n","----\n"," nput. The ouNput lantr gfed ,out the camddegram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for  \n","----\n","iter 19300, loss: 0.344251\n","----\n"," N assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hidden layer of 3 greute wastishe \"rs hall\" as in coas inputs f 3 che onte on  \n","----\n","iter 19400, loss: 0.331525\n","----\n"," ,e,l,o\"); We want the greenput and output layers, and a hidden lamer on (s input. The output layer contains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the gr \n","----\n","iter 19500, loss: 0.319709\n","----\n"," n example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters \"hell\" as i \n","----\n","iter 19600, loss: 0.308695\n","----\n"," layers, and a hidden layer of 3 units (neurons). This diagram shows the alayer of 3 unit layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the d in \n","----\n","iter 19700, loss: 0.298521\n","----\n"," his diagram shows the activations in the forward pass when the RNN is fed the charante RNN is fed the characters \"hell\" as input. The output layer contains confidences the RNN assigns for the next cha \n","----\n","iter 19800, loss: 0.289040\n","----\n"," ass when layer \"hill\" as input character (vocabulary is \"h,e,l,o\"); We want the green numbers to be high and output layers, and a hiratnshioas llin na).s thagre anucons ctivath 4-dimensionallw\"r of 3  \n","----\n","iter 19900, loss: 0.280206\n"]}],"source":["\n","n, p = 0, 0\n","mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n","mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n","smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n","while (n<20000):\n","  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n","    if p+seq_length+1 >= len(data) or n == 0: \n","        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n","        p = 0 # go from start of data\n","    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n","    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n","\n","  # sample from the model now and then\n","    if n % 100 == 0:\n","        sample_ix = sample(hprev, inputs[0], 200)\n","        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n","        print ('----\\n %s \\n----' % (txt, ))\n","\n","  # forward seq_length characters through the net and fetch gradient\n","    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n","    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","    if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n","    if (smooth_loss < 0.01):\n","        break\n","  \n","  # perform parameter update with Adagrad\n","    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n","                                [dWxh, dWhh, dWhy, dbh, dby], \n","                                [mWxh, mWhh, mWhy, mbh, mby]):\n","        mem += dparam * dparam\n","        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n","\n","    p += seq_length # move data pointer\n","    n += 1 # iteration counter"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":179.922658,"end_time":"2022-04-06T18:11:42.384025","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-04-06T18:08:42.461367","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}