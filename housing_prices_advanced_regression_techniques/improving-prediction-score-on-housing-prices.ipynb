{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Adapting from my own notebook here : https://www.kaggle.com/fanbyprinciple/beginner-pytorch-notebook-on-housing-dataset\nI feel I have forgotten stuff. This notebook was originally in part of the D2l book. https://d2l.ai/. Much recommended. Explanations of stuff taken from this book.","metadata":{}},{"cell_type":"markdown","source":"# LETS LOAD THE DATA","metadata":{}},{"cell_type":"markdown","source":"We use pandas to load the two csv files containing training and test data respectively.","metadata":{}},{"cell_type":"code","source":"import numpy\nimport pandas as pd\nimport torch\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-10T05:33:22.550874Z","iopub.execute_input":"2021-10-10T05:33:22.551268Z","iopub.status.idle":"2021-10-10T05:33:22.556635Z","shell.execute_reply.started":"2021-10-10T05:33:22.551232Z","shell.execute_reply":"2021-10-10T05:33:22.555267Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.558508Z","iopub.execute_input":"2021-10-10T05:33:22.559334Z","iopub.status.idle":"2021-10-10T05:33:22.617642Z","shell.execute_reply.started":"2021-10-10T05:33:22.559294Z","shell.execute_reply":"2021-10-10T05:33:22.616708Z"},"trusted":true},"execution_count":232,"outputs":[]},{"cell_type":"markdown","source":"The training dataset includes 1460 examples, 80 features, and 1 label, while the test data contains 1459 examples and 80 features.","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.618863Z","iopub.execute_input":"2021-10-10T05:33:22.619079Z","iopub.status.idle":"2021-10-10T05:33:22.646311Z","shell.execute_reply.started":"2021-10-10T05:33:22.619053Z","shell.execute_reply":"2021-10-10T05:33:22.645314Z"},"trusted":true},"execution_count":233,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.648592Z","iopub.execute_input":"2021-10-10T05:33:22.648968Z","iopub.status.idle":"2021-10-10T05:33:22.654041Z","shell.execute_reply.started":"2021-10-10T05:33:22.648926Z","shell.execute_reply":"2021-10-10T05:33:22.653476Z"},"trusted":true},"execution_count":234,"outputs":[]},{"cell_type":"markdown","source":"How to look at top 4 rows for columns","metadata":{}},{"cell_type":"code","source":"train_data.iloc[0:4, [1,2,3,-1,-2]]","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.655389Z","iopub.execute_input":"2021-10-10T05:33:22.655796Z","iopub.status.idle":"2021-10-10T05:33:22.674594Z","shell.execute_reply.started":"2021-10-10T05:33:22.655767Z","shell.execute_reply":"2021-10-10T05:33:22.673602Z"},"trusted":true},"execution_count":235,"outputs":[]},{"cell_type":"markdown","source":"train data has one column more for labels","metadata":{}},{"cell_type":"code","source":"len(test_data.columns), len(train_data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.676264Z","iopub.execute_input":"2021-10-10T05:33:22.676525Z","iopub.status.idle":"2021-10-10T05:33:22.687920Z","shell.execute_reply.started":"2021-10-10T05:33:22.676497Z","shell.execute_reply":"2021-10-10T05:33:22.686775Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"markdown","source":"# DATA PREPROCESSING\n\nsince we are going to use both of train data and test data through the preprocessing we first concatenate it. We can see that in each example, (the first feature is the ID.) This helps the model identify each training example. While this is convenient, it does not carry any information for prediction purposes. Hence, (we remove it from the dataset) before feeding the data into the model.","metadata":{}},{"cell_type":"code","source":"all_features = pd.concat((train_data.iloc[:,1:-1], test_data.iloc[:,1:]))\n\n# row wise concat","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.689553Z","iopub.execute_input":"2021-10-10T05:33:22.689847Z","iopub.status.idle":"2021-10-10T05:33:22.718076Z","shell.execute_reply.started":"2021-10-10T05:33:22.689816Z","shell.execute_reply":"2021-10-10T05:33:22.717164Z"},"trusted":true},"execution_count":237,"outputs":[]},{"cell_type":"code","source":"all_features.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.719345Z","iopub.execute_input":"2021-10-10T05:33:22.719606Z","iopub.status.idle":"2021-10-10T05:33:22.747732Z","shell.execute_reply.started":"2021-10-10T05:33:22.719577Z","shell.execute_reply":"2021-10-10T05:33:22.746752Z"},"trusted":true},"execution_count":238,"outputs":[]},{"cell_type":"markdown","source":"### 1. NORMALISATION\n\nNow lets look for numerical features, since we would want to normalise numerical features first.","metadata":{}},{"cell_type":"code","source":"all_features.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.751015Z","iopub.execute_input":"2021-10-10T05:33:22.751446Z","iopub.status.idle":"2021-10-10T05:33:22.759285Z","shell.execute_reply.started":"2021-10-10T05:33:22.751412Z","shell.execute_reply":"2021-10-10T05:33:22.758430Z"},"trusted":true},"execution_count":239,"outputs":[]},{"cell_type":"markdown","source":"We can use '==' on dataframes for comparison! here is how to find only numerical features.","metadata":{}},{"cell_type":"code","source":"all_features.dtypes[all_features.dtypes == 'int64']","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.760451Z","iopub.execute_input":"2021-10-10T05:33:22.760789Z","iopub.status.idle":"2021-10-10T05:33:22.777605Z","shell.execute_reply.started":"2021-10-10T05:33:22.760762Z","shell.execute_reply":"2021-10-10T05:33:22.776555Z"},"trusted":true},"execution_count":240,"outputs":[]},{"cell_type":"code","source":"numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\nall_features[numeric_features]","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.779006Z","iopub.execute_input":"2021-10-10T05:33:22.779246Z","iopub.status.idle":"2021-10-10T05:33:22.819295Z","shell.execute_reply.started":"2021-10-10T05:33:22.779220Z","shell.execute_reply":"2021-10-10T05:33:22.818333Z"},"trusted":true},"execution_count":241,"outputs":[]},{"cell_type":"code","source":"# numeric_features = all_features.dtypes[all_features.dtypes=='int64'].index","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.820678Z","iopub.execute_input":"2021-10-10T05:33:22.821001Z","iopub.status.idle":"2021-10-10T05:33:22.825199Z","shell.execute_reply.started":"2021-10-10T05:33:22.820963Z","shell.execute_reply":"2021-10-10T05:33:22.824253Z"},"trusted":true},"execution_count":242,"outputs":[]},{"cell_type":"markdown","source":"Notice the .index, it only returns the column name","metadata":{}},{"cell_type":"code","source":"def normalise(x):\n    return ((x - x.mean())/x.std())","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.826600Z","iopub.execute_input":"2021-10-10T05:33:22.826855Z","iopub.status.idle":"2021-10-10T05:33:22.837550Z","shell.execute_reply.started":"2021-10-10T05:33:22.826827Z","shell.execute_reply":"2021-10-10T05:33:22.836632Z"},"trusted":true},"execution_count":243,"outputs":[]},{"cell_type":"markdown","source":"Applying normalisation. \n\nAs stated above, we have a wide variety of data types. We will need to preprocess the data before we can start modeling. Let us start with the numerical features. First, we apply a heuristic, [replacing all missing values by the corresponding feature's mean.] Then, to put all features on a common scale, we (standardize the data by rescaling features to zero mean and unit variance):\n\n$$x \\leftarrow \\frac{x - \\mu}{\\sigma},$$\nwhere $\\mu$ and $\\sigma$ denote mean and standard deviation, respectively. To verify that this indeed transforms our feature (variable) such that it has zero mean and unit variance, note that $E[\\frac{x-\\mu}{\\sigma}] = \\frac{\\mu - \\mu}{\\sigma} = 0$ and that $E[(x-\\mu)^2] = (\\sigma^2 + \\mu^2) - 2\\mu^2+\\mu^2 = \\sigma^2$. Intuitively, we standardize the data for two reasons. First, it proves convenient for optimization. Second, because we do not know a priori which features will be relevant, we do not want to penalize coefficients assigned to one feature more than on any other.","metadata":{}},{"cell_type":"code","source":"# If test data were inaccessible, mean and standard deviation could be\n# calculated from training data\n\nall_features[numeric_features] = all_features[numeric_features].apply(\n    lambda x: (x - x.mean()) / (x.std()))\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.838947Z","iopub.execute_input":"2021-10-10T05:33:22.839222Z","iopub.status.idle":"2021-10-10T05:33:22.894123Z","shell.execute_reply.started":"2021-10-10T05:33:22.839192Z","shell.execute_reply":"2021-10-10T05:33:22.893151Z"},"trusted":true},"execution_count":244,"outputs":[]},{"cell_type":"markdown","source":"notice the difference in all_features before and after normalisation in `all_features[numeric_features]`","metadata":{}},{"cell_type":"code","source":"all_features[numeric_features]\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.895519Z","iopub.execute_input":"2021-10-10T05:33:22.895760Z","iopub.status.idle":"2021-10-10T05:33:22.932510Z","shell.execute_reply.started":"2021-10-10T05:33:22.895732Z","shell.execute_reply":"2021-10-10T05:33:22.931410Z"},"trusted":true},"execution_count":245,"outputs":[]},{"cell_type":"markdown","source":"### HANDLING MISSING ENTRIES\n\none of first jobs we have to do handlind datasets is to handle the missing values","metadata":{}},{"cell_type":"code","source":"all_features[numeric_features] = all_features[numeric_features].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.934391Z","iopub.execute_input":"2021-10-10T05:33:22.935004Z","iopub.status.idle":"2021-10-10T05:33:22.953423Z","shell.execute_reply.started":"2021-10-10T05:33:22.934948Z","shell.execute_reply":"2021-10-10T05:33:22.952422Z"},"trusted":true},"execution_count":246,"outputs":[]},{"cell_type":"markdown","source":"### ONE HOT ENCODING\n\ndiscrete features now need to be one hot encoded. The discrete columns are devided based on value, and 0 or 1 put in columns where the value is true, for example if `SaleType` had two discrete values other or WD, there would be two columns made `SaleType_other` and `Saletype_WD`","metadata":{}},{"cell_type":"code","source":"all_features = pd.get_dummies(all_features, dummy_na=True)\n\nall_features\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:22.954881Z","iopub.execute_input":"2021-10-10T05:33:22.955099Z","iopub.status.idle":"2021-10-10T05:33:23.035406Z","shell.execute_reply.started":"2021-10-10T05:33:22.955074Z","shell.execute_reply":"2021-10-10T05:33:23.034556Z"},"trusted":true},"execution_count":247,"outputs":[]},{"cell_type":"markdown","source":"notice that now there are 331 columns instead of earlier 25. Data prerpocessing is now over.","metadata":{}},{"cell_type":"markdown","source":"### BIFURCATING TEST AND TRAIN DATA\n\nBifurcating data back to train and test data. And converting them into float32 torch tensor. Via the values attribute, we can [extract the NumPy format from the pandas format and convert it into the tensor] representation for training.","metadata":{}},{"cell_type":"code","source":"n_train = len(train_data)\n\ntrain_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\ntest_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n\ntrain_labels = torch.tensor(train_data.iloc[:,-1].values, dtype=torch.float32)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.036491Z","iopub.execute_input":"2021-10-10T05:33:23.036793Z","iopub.status.idle":"2021-10-10T05:33:23.050476Z","shell.execute_reply.started":"2021-10-10T05:33:23.036768Z","shell.execute_reply":"2021-10-10T05:33:23.049632Z"},"trusted":true},"execution_count":248,"outputs":[]},{"cell_type":"markdown","source":"Since its the beginning we will go for a proof of concept and try and train a simple sequential model.\nLater we will change the definition of `get_net()`. To get started we train a linear model with squared loss. Not surprisingly, our linear model will not lead to a competition-winning submission but it provides a sanity check to see whether there is meaningful information in the data. If we cannot do better than random guessing here, then there might be a good chance that we have a data processing bug. And if things work, the linear model will serve as a baseline giving us some intuition about how close the simple model gets to the best reported models, giving us a sense of how much gain we should expect from fancier models.","metadata":{}},{"cell_type":"code","source":"loss = nn.MSELoss()\n\nin_features = train_features.shape[1]\nout_features = 1\n\ndef get_net():\n#     net = nn.Sequential(nn.Linear(in_features,256), nn.ReLU(), nn.Linear(256,out_features))\n    net = nn.Sequential(nn.Linear(in_features, out_features))\n    return net","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.053639Z","iopub.execute_input":"2021-10-10T05:33:23.053993Z","iopub.status.idle":"2021-10-10T05:33:23.059470Z","shell.execute_reply.started":"2021-10-10T05:33:23.053963Z","shell.execute_reply":"2021-10-10T05:33:23.058559Z"},"trusted":true},"execution_count":249,"outputs":[]},{"cell_type":"markdown","source":"With house prices, as with stock prices, we care about relative quantities more than absolute quantities. Thus [we tend to care more about the relative error $\\frac{y - \\hat{y}}{y}$] than about the absolute error $y - \\hat{y}$. For instance, if our prediction is off by USD 100,000 when estimating the price of a house in Rural Ohio, where the value of a typical house is 125,000 USD, then we are probably doing a horrible job. On the other hand, if we err by this amount in Los Altos Hills, California, this might represent a stunningly accurate prediction (there, the median house price exceeds 4 million USD).\n\n(One way to address this problem is to measure the discrepancy in the logarithm of the price estimates.) In fact, this is also the official error measure used by the competition to evaluate the quality of submissions. After all, a small value $\\delta$ for $|\\log y - \\log \\hat{y}| \\leq \\delta$ translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$. This leads to the following root-mean-squared-error between the logarithm of the predicted price and the logarithm of the label price:\n\n$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}.$$","metadata":{}},{"cell_type":"markdown","source":"### LOG RMSE MEASURE","metadata":{}},{"cell_type":"code","source":"def log_rmse(net, features, labels):\n    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\n    return rmse.item()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.060627Z","iopub.execute_input":"2021-10-10T05:33:23.060867Z","iopub.status.idle":"2021-10-10T05:33:23.072460Z","shell.execute_reply.started":"2021-10-10T05:33:23.060840Z","shell.execute_reply":"2021-10-10T05:33:23.071558Z"},"trusted":true},"execution_count":250,"outputs":[]},{"cell_type":"markdown","source":"We will create a helper function to load the dataset, and convert it into a data loader.","metadata":{}},{"cell_type":"code","source":"def load_array(data_array, batch_size):\n    in_dataset = torch.utils.data.TensorDataset(*data_array)\n    in_dataloader = torch.utils.data.DataLoader(in_dataset, shuffle=True, batch_size=batch_size)\n    return in_dataloader","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.074108Z","iopub.execute_input":"2021-10-10T05:33:23.074384Z","iopub.status.idle":"2021-10-10T05:33:23.082424Z","shell.execute_reply.started":"2021-10-10T05:33:23.074345Z","shell.execute_reply":"2021-10-10T05:33:23.081814Z"},"trusted":true},"execution_count":251,"outputs":[]},{"cell_type":"markdown","source":"Helper function for initialisation of weights.","metadata":{}},{"cell_type":"code","source":"def init_weights(m):\n    if type(m)==nn.Linear or type(m)==nn.Conv2d:\n        nn.init.xavier_uniform_(m.weight)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.083247Z","iopub.execute_input":"2021-10-10T05:33:23.083476Z","iopub.status.idle":"2021-10-10T05:33:23.092835Z","shell.execute_reply.started":"2021-10-10T05:33:23.083451Z","shell.execute_reply":"2021-10-10T05:33:23.092097Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"markdown","source":"Accuracy is not meaningful in case of regression, because you cannot expect the model to make a prediction of a house that is exactly same as that of the test value. Still we I put it here, out of habit. ","metadata":{}},{"cell_type":"code","source":"def accuracy(y_hat, y):\n    return (torch.argmax(y_hat, dim=1)==y).sum().float().mean()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.093980Z","iopub.execute_input":"2021-10-10T05:33:23.094203Z","iopub.status.idle":"2021-10-10T05:33:23.102918Z","shell.execute_reply.started":"2021-10-10T05:33:23.094169Z","shell.execute_reply":"2021-10-10T05:33:23.102080Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"markdown","source":"We need to put both our model and inputs onto gpu for faster calculation. Thats what we are doing here. We will use `torch.device('cuda)` for all the calculations.","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.106270Z","iopub.execute_input":"2021-10-10T05:33:23.106539Z","iopub.status.idle":"2021-10-10T05:33:23.115593Z","shell.execute_reply.started":"2021-10-10T05:33:23.106513Z","shell.execute_reply":"2021-10-10T05:33:23.114520Z"},"trusted":true},"execution_count":254,"outputs":[]},{"cell_type":"markdown","source":"As simple as they come.","metadata":{}},{"cell_type":"code","source":"net = get_net()\nnet","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.118302Z","iopub.execute_input":"2021-10-10T05:33:23.119219Z","iopub.status.idle":"2021-10-10T05:33:23.130838Z","shell.execute_reply.started":"2021-10-10T05:33:23.119189Z","shell.execute_reply":"2021-10-10T05:33:23.129683Z"},"trusted":true},"execution_count":255,"outputs":[]},{"cell_type":"markdown","source":"### HYPERPARAMETERS","metadata":{}},{"cell_type":"code","source":"test_labels = None\nbatch_size= 64\nlearning_rate = 0.03\nlr = learning_rate\nweight_decay = 0\nnum_epochs = 100","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.132523Z","iopub.execute_input":"2021-10-10T05:33:23.133222Z","iopub.status.idle":"2021-10-10T05:33:23.139406Z","shell.execute_reply.started":"2021-10-10T05:33:23.133179Z","shell.execute_reply":"2021-10-10T05:33:23.138610Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"markdown","source":"Looking inside train dataloader what we are actually working with, you can totally skip the next cell.","metadata":{}},{"cell_type":"code","source":"train_dataloader = load_array((train_features, train_labels), batch_size)\nwith torch.no_grad():\n    for X, y in train_dataloader:\n        print(X.shape)\n        print(X, \"\\n\")\n        print(len(y))\n        print(y, \"\\n\")\n        y_hat = net(X)\n        print(y_hat)\n        \n        print(accuracy(y_hat, y))\n        #for x in X:  \n        #    y_hat = net(x)\n        #    print(x)\n        #    print(y_hat)\n        #    print(y_hat.shape)\n        break","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:37:38.989248Z","iopub.execute_input":"2021-10-10T05:37:38.989576Z","iopub.status.idle":"2021-10-10T05:37:39.010576Z","shell.execute_reply.started":"2021-10-10T05:37:38.989545Z","shell.execute_reply":"2021-10-10T05:37:39.009516Z"},"trusted":true},"execution_count":264,"outputs":[]},{"cell_type":"markdown","source":"Looks good onto..\n\n## TRAINING","metadata":{}},{"cell_type":"code","source":"def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay,batch_size):\n    train_ls, test_ls = [], []\n    \n    net = net.to(device)\n    net.apply(init_weights)\n    \n    train_acc = []\n    \n    \n    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, weight_decay=weight_decay)\n    \n    train_dataloader = load_array((train_features, train_labels),batch_size)\n    \n    for epoch in range(num_epochs):\n        curr_acc = 0\n        numer = 0\n        for X, y in train_dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            \n            y_hat = net(X)\n            \n            l = loss(y_hat, y.unsqueeze(1))\n            curr_acc += accuracy(y_hat, y)\n            \n#             print(y)\n#             print(y_hat)\n            \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            numer += len(y)\n            \n        train_ls.append(log_rmse(net, train_features, train_labels))\n        train_acc.append(curr_acc/ numer)\n        if test_labels is not None:\n            test_ls.append(log_rmse(net, test_features, test_labels))\n        print(f'for epoch {epoch} rmse: {train_ls[-1]}, accuracy : {train_acc[-1]}')\n    \n    return train_ls, test_ls\n            ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.160067Z","iopub.execute_input":"2021-10-10T05:33:23.160718Z","iopub.status.idle":"2021-10-10T05:33:23.171304Z","shell.execute_reply.started":"2021-10-10T05:33:23.160685Z","shell.execute_reply":"2021-10-10T05:33:23.170412Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"net(train_features).detach()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.172780Z","iopub.execute_input":"2021-10-10T05:33:23.173049Z","iopub.status.idle":"2021-10-10T05:33:23.186417Z","shell.execute_reply.started":"2021-10-10T05:33:23.173018Z","shell.execute_reply":"2021-10-10T05:33:23.185633Z"},"trusted":true},"execution_count":259,"outputs":[]},{"cell_type":"code","source":"#train_ls, test_ls =  train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay,batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.187626Z","iopub.execute_input":"2021-10-10T05:33:23.190510Z","iopub.status.idle":"2021-10-10T05:33:23.195068Z","shell.execute_reply.started":"2021-10-10T05:33:23.190466Z","shell.execute_reply":"2021-10-10T05:33:23.194307Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"markdown","source":"### PLOTTING\n\nI'll be using D2l.ai plot for doing the visualisation part. so lets install it.","metadata":{}},{"cell_type":"code","source":"!pip install -U d2l\nimport d2l\nfrom d2l import torch\nfrom d2l.torch import *","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:23.196095Z","iopub.execute_input":"2021-10-10T05:33:23.196809Z","iopub.status.idle":"2021-10-10T05:33:31.870539Z","shell.execute_reply.started":"2021-10-10T05:33:23.196767Z","shell.execute_reply":"2021-10-10T05:33:31.869517Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"markdown","source":"### Prediction\n\nTechnically, its train and prediction both together in one function. Since we would be using it again and again.","metadata":{}},{"cell_type":"code","source":"def train_and_pred(train_features, test_feature, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size):\n    net = get_net()\n    train_ls, _ = train(net, train_features, train_labels, None, None,\n    num_epochs, lr, weight_decay, batch_size)\n    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',\n    ylabel='log rmse', xlim=[1, num_epochs], yscale='log')\n    print(f'train log rmse {float(train_ls[-1]):f}')\n    # Apply the network to the test set\n    preds = net(test_features).detach().numpy()\n    # Reformat it to export to Kaggle\n    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:31.871973Z","iopub.execute_input":"2021-10-10T05:33:31.872207Z","iopub.status.idle":"2021-10-10T05:33:31.884596Z","shell.execute_reply.started":"2021-10-10T05:33:31.872177Z","shell.execute_reply":"2021-10-10T05:33:31.883462Z"},"trusted":true},"execution_count":262,"outputs":[]},{"cell_type":"code","source":"train_and_pred(train_features, test_features, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T05:33:31.885796Z","iopub.execute_input":"2021-10-10T05:33:31.886303Z","iopub.status.idle":"2021-10-10T05:33:36.093713Z","shell.execute_reply.started":"2021-10-10T05:33:31.886270Z","shell.execute_reply":"2021-10-10T05:33:36.092493Z"},"trusted":true},"execution_count":263,"outputs":[]},{"cell_type":"markdown","source":"rmse of 4.08202 takes us to 4k rank in leaderboard. Lets try and use a different get_net function. Notice that accuracy is 0.0 because y_hat predicted is never actually equal to the test value y.","metadata":{}},{"cell_type":"markdown","source":"# IMPROVING OUR POSITION ON LEADERBOARD","metadata":{}},{"cell_type":"markdown","source":"implementation of densenet: mentioned in this model\n\nhttps://arxiv.org/pdf/2108.00864.pdf","metadata":{}},{"cell_type":"code","source":"def get_net():\n    net = nn.Sequential(nn.Linear(in_features, 256), nn.ReLU(), nn.Linear(256,out_features))\n    return net\n\nnet = get_net()\nnet","metadata":{"execution":{"iopub.status.busy":"2021-10-10T06:41:25.225611Z","iopub.execute_input":"2021-10-10T06:41:25.225956Z","iopub.status.idle":"2021-10-10T06:41:25.237589Z","shell.execute_reply.started":"2021-10-10T06:41:25.225920Z","shell.execute_reply":"2021-10-10T06:41:25.236588Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"train_and_pred(train_features, test_features, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T06:41:31.004861Z","iopub.execute_input":"2021-10-10T06:41:31.005207Z","iopub.status.idle":"2021-10-10T06:41:39.740232Z","shell.execute_reply.started":"2021-10-10T06:41:31.005172Z","shell.execute_reply":"2021-10-10T06:41:39.739484Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"markdown","source":"Now this takes you within earshot of 1k in leaderboard. Our best score yet!","metadata":{}},{"cell_type":"code","source":"#would creating a more complex model help?\n\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features, 256), nn.ReLU(),nn.Linear(256,64), nn.ReLU(), nn.Linear(64,out_features))\n    return net\n\nnet = get_net()\nnet","metadata":{"execution":{"iopub.status.busy":"2021-10-10T06:48:04.245188Z","iopub.execute_input":"2021-10-10T06:48:04.245561Z","iopub.status.idle":"2021-10-10T06:48:04.256858Z","shell.execute_reply.started":"2021-10-10T06:48:04.245526Z","shell.execute_reply":"2021-10-10T06:48:04.255699Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"train_and_pred(train_features, test_features, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T06:48:07.715568Z","iopub.execute_input":"2021-10-10T06:48:07.715885Z","iopub.status.idle":"2021-10-10T06:48:17.583345Z","shell.execute_reply.started":"2021-10-10T06:48:07.715854Z","shell.execute_reply":"2021-10-10T06:48:17.582361Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"markdown","source":"not really it didnt quite help. The score is worse than before. Hmm.","metadata":{}},{"cell_type":"markdown","source":"This gives me an idea: https://towardsdatascience.com/pitfalls-with-dropout-and-batchnorm-in-regression-problems-39e02ce08e4d, can we use VGG models with head for linear regression?","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-10T07:54:44.242184Z","iopub.execute_input":"2021-10-10T07:54:44.242790Z","iopub.status.idle":"2021-10-10T07:54:51.158894Z","shell.execute_reply.started":"2021-10-10T07:54:44.242610Z","shell.execute_reply":"2021-10-10T07:54:51.158143Z"},"trusted":true},"execution_count":1,"outputs":[]}]}